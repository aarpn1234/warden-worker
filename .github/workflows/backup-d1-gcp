name: Backup D1 Database to S3

on:
  schedule:
    - cron: "0 6 * * *"
  workflow_dispatch:

env:
  BACKUP_RETENTION_DAYS: 30

jobs:
  backup-production:
    name: Backup Production D1 Database
    runs-on: ubuntu-latest

    steps:
      - name: Get current date
        id: date
        run: echo "date=$(date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_OUTPUT

      - name: Install wrangler
        run: npm install -g wrangler

      - name: Get D1 Database Name
        id: db_name
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          DB_NAME=$(npx wrangler d1 list --json | jq -r '.[] | select(.uuid == "${{ secrets.D1_DATABASE_ID }}") | .name')
          if [ -z "$DB_NAME" ]; then
            echo "❌ Error: Could not find database with D1_DATABASE_ID"
            exit 1
          fi
          echo "db_name=$DB_NAME" >> $GITHUB_OUTPUT

      - name: Export D1 Database
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "Exporting D1 database..."
          npx wrangler d1 export "${{ steps.db_name.outputs.db_name }}" \
            --remote \
            --output=backup.sql

          gzip backup.sql
          echo "Backup compressed: backup.sql.gz"
          ls -lh backup.sql.gz

      - name: Encrypt backup (optional)
        env:
          BACKUP_ENCRYPTION_KEY: ${{ secrets.BACKUP_ENCRYPTION_KEY }}
        run: |
          if [ -n "$BACKUP_ENCRYPTION_KEY" ]; then
            echo "Encrypting backup with AES-256..."
            openssl enc -aes-256-cbc -salt -pbkdf2 -iter 100000 \
              -in backup.sql.gz \
              -out "vault1_prod_${{ steps.date.outputs.date }}.sql.gz.enc" \
              -pass pass:"$BACKUP_ENCRYPTION_KEY"
            rm backup.sql.gz
          else
            mv backup.sql.gz "vault1_prod_${{ steps.date.outputs.date }}.sql.gz"
          fi

      # ------------------------------------------------------
      # Google Cloud authentication
      # ------------------------------------------------------

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Setup gcloud CLI
        uses: google-github-actions/setup-gcloud@v2
        with:
          install_components: gsutil

      - name: Upload to GCP Cloud Storage
        env:
          BACKUP_ENCRYPTION_KEY: ${{ secrets.BACKUP_ENCRYPTION_KEY }}
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
        run: |
          if [ -n "$BACKUP_ENCRYPTION_KEY" ]; then
            FILE_NAME="vault1_prod_${{ steps.date.outputs.date }}.sql.gz.enc"
          else
            FILE_NAME="vault1_prod_${{ steps.date.outputs.date }}.sql.gz"
          fi

          echo "Uploading $FILE_NAME to Google Cloud Storage..."
          gsutil cp "$FILE_NAME" "gs://$GCS_BUCKET/$FILE_NAME"
          echo "✅ Upload complete."

      - name: Clean up old backups in GCP Cloud Storage
        env:
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
          BACKUP_RETENTION_DAYS: ${{ env.BACKUP_RETENTION_DAYS }}
        run: |
          CUTOFF_DATE=$(date -d "-${BACKUP_RETENTION_DAYS} days" +%Y-%m-%d)
          echo "Cleaning up backups older than $CUTOFF_DATE..."

          gsutil ls -l "gs://$GCS_BUCKET/" | while read -r SIZE DATETIME FILENAME; do
            # Skip TOTAL line and empty lines
            if [[ "$SIZE" == "TOTAL" ]] || [[ -z "$FILENAME" ]]; then
              continue
            fi

            FILE_DATE=$(echo "$DATETIME" | cut -dT -f1)

            if [[ "$FILE_DATE" < "$CUTOFF_DATE" ]]; then
              echo "Deleting old backup: $FILENAME (date: $FILE_DATE)"
              gsutil rm "$FILENAME"
            fi
          done

          echo "✅ Old backup cleanup complete."
